# -*- coding: utf-8 -*-
"""heart-failure-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l0hOaN28ZE3uVA10r6FTSXW1mBB0WyuZ

## Personal Data

* Name  : Adil Faruq Habibi
* Email : adilfaruqhabibi@gmail.com
* Phone : +6285265408417
* Kota  : Kota Jakarta Timur, DKI Jakarta

## Installing Libraries
"""

# Download All Dependencies
!pip install -q kaggle

# updating (server 1.5.12 / client 1.5.4)
!pip install --upgrade --force-reinstall --no-deps kaggle

!pip install --upgrade matplotlib

"""## Import Required Library"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from IPython.display import display
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
from sklearn.metrics import f1_score, jaccard_score, confusion_matrix, classification_report

"""## Data Collection"""

# Upload API Files from Kaggle
from google.colab import files
files.upload()

# Connect Collab to Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

# Searching heart disease datasets
! kaggle datasets list -s heart

# download dataset heart-failure-prediction from fedesoriano
!kaggle datasets download "fedesoriano/heart-failure-prediction"

# unzip dataset
!unzip heart-failure-prediction.zip

"""## Data Loading"""

# buat pandas dataframe
data = pd.read_csv('heart.csv')
data.head()

data.info()

data.describe()

"""## Data Cleaning

Berdasarkan informasi dari https://www.healthline.com/health/serum-cholesterol, ditemukan bahwa kandungan serum cholesterol tidak memungkinkan untuk bernilai 0 md/dL. Sehingga disimpulkan nilai 0 pada data adalah missing value
"""

# Cek missing values untuk kolom cholesterol
print((data.Cholesterol == 0).sum())

# distribusi data cholesterol ketika terdapat missing values
plt.hist(data[data.HeartDisease==1]['Cholesterol'], density=True, label='Heart Disease', alpha=0.7)
plt.hist(data[data.HeartDisease==0]['Cholesterol'], density=True, label='No Disease', alpha=0.7)
plt.legend()
plt.show()

# impute missing values dengan strategi mean
data['Cholesterol'] = data.Cholesterol.replace(0, np.nan)
data['Cholesterol'] = data.Cholesterol.fillna(data.groupby('HeartDisease').Cholesterol.transform('mean'))
pd.DataFrame(data.Cholesterol.describe()).T

print((data.Cholesterol == 0).sum())

# distribusi data cholesterol terbaru
plt.hist(data[data.HeartDisease==1]['Cholesterol'], density=True, label='Heart Disease', alpha=0.7)
plt.hist(data[data.HeartDisease==0]['Cholesterol'], density=True, label='No Disease', alpha=0.7)
plt.legend()
plt.show()

"""## EDA"""

# jumlah data masing-masing kelas
bar_plot = plt.barh(
    np.unique(data.HeartDisease).astype('str'),
    data.HeartDisease.value_counts()
)
plt.bar_label(bar_plot)
plt.title('Jumlah Data Untuk Setiap Kelas')
plt.yticks(labels=['Heart Disease', 'No Disease'], ticks=[1,0])
plt.show()

# Pembagian label fitur kategorikal dan numerikal
categorical_features = data.select_dtypes(include=['object']).columns.tolist() + ['FastingBS']
numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_features.remove('FastingBS')
numerical_features.remove('HeartDisease')

print('Categorical features : \n', categorical_features, '\n')
print('numerical features : \n', numerical_features)

# explorasi categorical features
for feature in categorical_features:
  count = data[feature].value_counts()
  percent = 100*data[feature].value_counts(normalize=True)
  df = pd.DataFrame({'jumlah sampel':count, 'proporsi(%)':percent.round(1)})
  count.plot(kind='bar', title=feature, stacked=False)
  plt.show()
  display(df)
  print('\n\n')

# Persentase tiap kelas terhadap kategorikal
for feature in categorical_features:
  idxes = np.unique(data[feature])
  fig, ax = plt.subplots(1, len(idxes), figsize=(15,4))
  fig.suptitle('Persentase tiap kelas terhadap fitur - '+feature)
  for i, idx in enumerate(idxes):
    bar_plot = ax[i].bar(
        ['Heart Disease', 'No Disease'],
        data.groupby(feature)['HeartDisease'].value_counts(normalize=True)[idx] * 100
    )
    ax[i].set_xlabel(idx)
    ax[i].bar_label(bar_plot, fmt='%.2f%%')
    fig.show()

numerical_features

# Dsitribusi dan cek outlier tiap kelas terhadap numerikal fitur
for feature in numerical_features:
  fig, ax = plt.subplots(1, 2, figsize=(10,5))
  fig.suptitle('Distribusi berdasarkan - '+feature)
  ax[0].hist(data[data.HeartDisease==1][feature], label='Heart Disease', alpha=0.6, density=True)
  ax[0].hist(data[data.HeartDisease==0][feature], label='No Disease', alpha=0.6, density=True)
  ax[0].set_xlabel(feature)
  ax[0].set_ylabel('Probability Density')
  ax[0].legend()

  sns.boxplot(data=[data[data.HeartDisease==1][feature],data[data.HeartDisease==0][feature]], ax=ax[1])
  ax[1].set_xticklabels(['Heart Disease', 'No Disease'])
  ax[1].set_ylabel(feature)

  fig.show()

# bersihkan data dari outlier
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR=Q3-Q1
data=data[~((data<(Q1-1.5*IQR))|(data>(Q3+1.5*IQR))).any(axis=1)]
 
# cek ukuran dataset setelah kita drop outliers
data.shape

# Cek Korelasi dengan Heatmap
plt.figure(figsize=(8,6))
sns.heatmap(data[numerical_features+['HeartDisease']].corr(), annot=True,fmt='.4g')
plt.show()

# bagi fitur dan target
y = data.HeartDisease
X = data.drop('HeartDisease', axis=1)
X = pd.get_dummies(X, columns=categorical_features, drop_first=True)

# mengurangi dimensi dengan PCA dengan pipeline dari data yang distandarisasi

# inisialisasi instances scaler dan pca
scaler = MinMaxScaler()
pca = PCA(random_state=123)

# buat pipline
pipeline = make_pipeline(scaler, pca)

# latih pipeline terhadap sample
pipeline.fit(X)

# ekstrak jumlah komponen yang digunakan
features = range(pca.n_components_)

# tampilkan secara grafis the explained variances
plt.figure(figsize=(12,5))
bar_plot = plt.bar(features, pca.explained_variance_)
plt.xticks(features)
plt.xlabel('PCA Features')
plt.ylabel('variance')
plt.bar_label(bar_plot, fmt='%.3f')
plt.show()

# tampilkan secara grafis rasio dari the explained variances
plt.figure(figsize=(12,5))
bar_plot = plt.bar(features, pca.explained_variance_ratio_.round(3)*100)
plt.xticks(features)
plt.xlabel('PCA Features')
plt.ylabel('variance ratio (%)')
plt.bar_label(bar_plot, fmt='%.2f%%')
plt.show()

# inisialisasi pca instences dengan jumlah komponen sebanyak 4
pca = PCA(n_components=4)

# normalisasi features
scaled_X = scaler.fit_transform(X)

# fit dan transform features
pca.fit(scaled_X)
pca_features = pca.transform(scaled_X)

# dimensi sesudah dan sebelum direduksi
pca_features.shape, scaled_X.shape

# bagi data training dan testing
X_train, X_test, y_train, y_test = train_test_split(pca_features, y, test_size=0.2, random_state=123)

"""## Model Selection

> KNN
"""

knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

"""> RandomForest"""

rf = RandomForestClassifier(n_estimators=70, max_depth=16, max_leaf_nodes=20, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

"""> AdaBoost"""

boosting = AdaBoostClassifier(random_state=42)                             
boosting.fit(X_train, y_train)
y_pred_boosting = boosting.predict(X_test)

"""> SVM"""

svc = svm.SVC(kernel='sigmoid', gamma='auto')
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)

# Buat dataframe berisikan score masing-masing algoritma dari data train dan test
score = pd.DataFrame()
 
# buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': rf, 'Boosting': boosting, 'SVM': svc}
 
# hitung f1-score masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
  score.loc[name, 'train'] = f1_score(y_train, y_pred=model.predict(X_train), average='weighted')
  score.loc[name, 'test'] = f1_score(y_test, y_pred=model.predict(X_test), average='weighted')
  
# Panggil score
score

"""> Model yang dipilih adalah AdaBoost. Ini dikarenakan bias yang lebih rendah apabila dibandingkan dengan RandomForest"""

# visualisasi score pada masing-masing algoritma terhadap data train dan test
fig, ax = plt.subplots(figsize=(8,6))
score.sort_values(by='test', ascending=False).plot.barh(ax=ax, width=0.7, zorder=3)
for container in ax.containers:
  ax.bar_label(container, fmt='%.2f')
plt.title('f1-score tiap-tiap algoritma dari train dan test sets', fontsize=15)
ax.legend(bbox_to_anchor=(1,1))
plt.show()

"""# Model Evaluation"""

# hitung confusion matrix dari data testing
cnf_matrix = confusion_matrix(y_test, y_pred_boosting)

# Plot confusion matrix
sns.heatmap(cnf_matrix, annot=True, cbar=False, cmap='Blues')
plt.title('Confusion Matrix', fontsize=15)
plt.show()

print(classification_report(y_test, y_pred_boosting))

prediksi = X_test[:5].copy()
pred_dict = {'y_true':y_test[:5]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""Hasil ujicoba tiap model terhadap 5 buah data menghasilkan satu buah kesalah prediksi (pada indeks: 188)"""